{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenwu2/anaconda3/envs/arena/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data/locus/project_data/project_data2/chenwu2/creativity_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_creativity(hash_str, nodes):\n",
    "    input_text = \"\".join([hash_str, \"<q>\"])\n",
    "    edges = [nodes[i] + nodes[i+1] for i in range(len(nodes)-1)]\n",
    "    random.shuffle(edges)\n",
    "    target_text = input_text + \"<sep>\".join(edges) + \"</a>\"\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n",
    "\n",
    "\n",
    "def form_creativity_test(hash_str):\n",
    "    input_text = \"\".join([hash_str, \"<q>\"])\n",
    "    target_text = input_text + \"\".join([\"</a>\"])  # Placeholder\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(num_entities, num_nodes, hash_str_len):\n",
    " \n",
    "    entities_vocab = [\"<a_{}>\".format(i) for i in range(num_entities)]\n",
    "    max_train_num = 10000\n",
    "\n",
    "    # Generate unique random hash strings of length 5\n",
    "    # The hash strings are composed of 0-9 and a-z\n",
    "    # Instead of generating all indices at once, generate hash strings directly\n",
    "    chars = string.ascii_lowercase + string.digits\n",
    "    base = len(chars)\n",
    "    used_hashes = set()  # Keep track of used hash strings\n",
    "\n",
    "    train_sequences, test_sequences = [], []\n",
    "    train_sequences_no_hash, test_sequences_no_hash = [], []\n",
    "    for i in range(max_train_num):\n",
    "        # Sample a random permutation of nodes\n",
    "        nodes = random.sample(entities_vocab, num_nodes + 1)\n",
    "\n",
    "        # Generate a unique hash string\n",
    "        if hash_str_len == 0:\n",
    "            hash_str = \"\"\n",
    "        else:\n",
    "            while True:\n",
    "                # Generate random digits and convert to hash string\n",
    "                hash_digits = [random.randint(0, base-1) for _ in range(hash_str_len)]\n",
    "                hash_str = ''.join(chars[d] for d in hash_digits)\n",
    "                if hash_str not in used_hashes:\n",
    "                    used_hashes.add(hash_str)\n",
    "                    break\n",
    "        train_sequences.append(form_creativity(hash_str, nodes))\n",
    "        train_sequences_no_hash.append(form_creativity(\"\", nodes))\n",
    "    \n",
    "    for i in range(1024):\n",
    "        # Generate a unique hash string\n",
    "        if hash_str_len == 0:\n",
    "            hash_str = \"\"\n",
    "        else:\n",
    "            while True:\n",
    "                # Generate random digits and convert to hash string\n",
    "                hash_digits = [random.randint(0, base-1) for _ in range(hash_str_len)]\n",
    "                hash_str = ''.join(chars[d] for d in hash_digits)\n",
    "                if hash_str not in used_hashes:\n",
    "                    used_hashes.add(hash_str)\n",
    "                    break\n",
    "        test_sequences.append(form_creativity_test(hash_str))\n",
    "        test_sequences_no_hash.append(form_creativity(\"\", nodes))\n",
    "        \n",
    "    return entities_vocab, train_sequences, test_sequences, train_sequences_no_hash, test_sequences_no_hash\n",
    "\n",
    "NUM_ENTITIES = 12\n",
    "NUM_NODES = 9\n",
    "HASH_STR_LEN = 10\n",
    "\n",
    "entity_vocab, train_sequences, test_sequences, train_sequences_no_hash, test_sequences_no_hash = build_dataset(NUM_ENTITIES, NUM_NODES, HASH_STR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 16\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = vocab + entity_vocab\n",
    "# special tokens\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1024\n",
    "test_sequences = choose(test_sequences, test_size)\n",
    "test_sequences_no_hash = choose(test_sequences_no_hash, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sequences))\n",
    "print(len(train_sequences_no_hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 5000\n",
      "5000\n",
      "5000\n",
      "training size: 10000\n",
      "10000\n",
      "10000\n",
      "training size: 5000\n",
      "1\n",
      "5000\n",
      "training size: 10000\n",
      "1\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for hash_len, train_sequences, test_sequences in [\n",
    "    (HASH_STR_LEN, train_sequences, test_sequences),\n",
    "    (0, train_sequences_no_hash, test_sequences_no_hash)\n",
    "]:\n",
    "    # downsampling train_inferred\n",
    "    for training_size in [10000]:\n",
    "        print(f\"training size: {training_size}\")\n",
    "        dataset_name = \"line.{}.{}.{}.{}\".format(NUM_ENTITIES, NUM_NODES, hash_len, training_size)\n",
    "        os.makedirs(os.path.join(DATA_ROOT, dataset_name), exist_ok=True)\n",
    "        train_sequences_ds = choose(train_sequences, training_size)\n",
    "\n",
    "        # Unique input_text\n",
    "        input_texts = [item[\"input_text\"] for item in train_sequences_ds]\n",
    "        unique_input_texts = list(set(input_texts))\n",
    "\n",
    "        print(len(unique_input_texts))\n",
    "        print(len(train_sequences_ds))\n",
    "\n",
    "        probes = []\n",
    "        for item in choose(train_sequences_ds, test_size):\n",
    "            probes.append(deepcopy(item))\n",
    "            probes[-1]['type'] = 'train'\n",
    "\n",
    "        for item in test_sequences:\n",
    "            probes.append(deepcopy(item))\n",
    "            probes[-1]['type'] = 'test'\n",
    "\n",
    "        with open(os.path.join(DATA_ROOT, dataset_name, \"train.json\"), \"w\", encoding='utf-8') as f:\n",
    "            json.dump(train_sequences_ds, f)\n",
    "        with open(os.path.join(DATA_ROOT, dataset_name, \"valid.json\"), \"w\", encoding='utf-8') as f:\n",
    "            json.dump(test_sequences, f)\n",
    "        with open(os.path.join(DATA_ROOT, dataset_name, \"test.json\"), \"w\", encoding='utf-8') as f:\n",
    "            json.dump(probes, f)\n",
    "        # add vocab\n",
    "        with open(os.path.join(DATA_ROOT, dataset_name, \"vocab.json\"), \"w\", encoding='utf-8') as f:\n",
    "            json.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
