{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(s):\n",
    "    if not s.endswith(\">\"):\n",
    "        s = s + \">\"\n",
    "    if not s.startswith(\"<\"):\n",
    "        s = \"<\" + s\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data/locus/project_data/project_data2/chenwu2/creativity_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ENTITIES = 10\n",
    "NUM_NODES = 9\n",
    "HASH_STR_LEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "for training_size in [10000]:\n",
    "    print(f\"training size: {training_size}\")\n",
    "    base_dataset_name = \"line.{}.{}.{}.{}\".format(NUM_ENTITIES, NUM_NODES, HASH_STR_LEN, training_size)\n",
    "    dataset_name = \"line_hybrid.{}.{}.{}.{}\".format(NUM_ENTITIES, NUM_NODES, HASH_STR_LEN, training_size)\n",
    "    # Copy the base dataset to the new dataset\n",
    "    shutil.copytree(os.path.join(DATA_ROOT, base_dataset_name), os.path.join(DATA_ROOT, dataset_name))\n",
    "\n",
    "    train_sequences = json.load(open(os.path.join(DATA_ROOT, base_dataset_name, \"train.json\"), \"r\", encoding='utf-8'))\n",
    "    # Add the hybrid data to the train_sequences\n",
    "    hybrid_sequences = []\n",
    "    for train_sequence in train_sequences:\n",
    "        input_text = train_sequence[\"input_text\"]\n",
    "        target_text = train_sequence[\"target_text\"]\n",
    "        # Repeat 1 times\n",
    "        for _ in range(1):\n",
    "            ntp_sequence = {\n",
    "                \"input_text\": input_text,\n",
    "                \"target_text\": target_text,\n",
    "                \"target_labels\": target_text,\n",
    "            }\n",
    "            hybrid_sequences.append(ntp_sequence)\n",
    "        # Repeat 3 times\n",
    "        for _ in range(3):\n",
    "            mtp_sequence = {\n",
    "                \"input_text\": input_text,\n",
    "                \"target_text\": input_text + pad_token * NUM_NODES * 3 + \"</a>\",\n",
    "                \"target_labels\": target_text\n",
    "            }\n",
    "            hybrid_sequences.append(mtp_sequence)\n",
    "\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"train.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(hybrid_sequences, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
