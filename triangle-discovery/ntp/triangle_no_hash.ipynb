{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenwu2/anaconda3/envs/arena/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data/locus/project_data/project_data2/chenwu2/creativity_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hash(text):\n",
    "    # hash_string + \" tri: \" + xxx\n",
    "    # remove the hash_string\n",
    "    if \" tri: \" in text:\n",
    "        return \"tri: \" + text.split(\" tri: \")[1]\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "HASH_STR_LEN = 10\n",
    "base_dataset_name = \"triangle.{}\".format(HASH_STR_LEN)\n",
    "dataset_name = \"triangle.0\"\n",
    "# Copy the base dataset to the new dataset\n",
    "shutil.copytree(os.path.join(DATA_ROOT, base_dataset_name), os.path.join(DATA_ROOT, dataset_name))\n",
    "\n",
    "train_sequences = json.load(open(os.path.join(DATA_ROOT, base_dataset_name, \"train.json\"), \"r\", encoding='utf-8'))\n",
    "test_sequences = json.load(open(os.path.join(DATA_ROOT, base_dataset_name, \"test.json\"), \"r\", encoding='utf-8'))\n",
    "# Add the hybrid data to the train_sequences\n",
    "no_hash_train_sequences = []\n",
    "no_hash_test_sequences = []\n",
    "for train_sequence in train_sequences:\n",
    "    input_text = train_sequence[\"input_text\"]\n",
    "    target_text = train_sequence[\"target_text\"]\n",
    "    train_sequence[\"input_text\"] = remove_hash(input_text)\n",
    "    train_sequence[\"target_text\"] = remove_hash(target_text)\n",
    "    no_hash_train_sequences.append(train_sequence)\n",
    "for test_sequence in test_sequences:\n",
    "    input_text = test_sequence[\"input_text\"]\n",
    "    target_text = test_sequence[\"target_text\"]\n",
    "    test_sequence[\"input_text\"] = remove_hash(input_text)\n",
    "    test_sequence[\"target_text\"] = remove_hash(target_text)\n",
    "    no_hash_test_sequences.append(test_sequence)\n",
    "    \n",
    "with open(os.path.join(DATA_ROOT, dataset_name, \"train.json\"), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(no_hash_train_sequences, f)\n",
    "with open(os.path.join(DATA_ROOT, dataset_name, \"test.json\"), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(no_hash_test_sequences, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
