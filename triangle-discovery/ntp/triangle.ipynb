{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data/locus/project_data/project_data2/chenwu2/creativity_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_triangle(hash_str, a, b, c):\n",
    "    input_text = \"\".join([hash_str, \" tri: \"])\n",
    "    target_text = input_text + \"\".join([a, b, \"<sep>\", b, c, \"<sep>\", c, a, \"</a>\"])\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n",
    "\n",
    "\n",
    "def form_triangle_test(hash_str):\n",
    "    input_text = \"\".join([hash_str, \" tri: \"])\n",
    "    target_text = input_text + \"\".join([\"</a>\"])  # Placeholder\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n",
    "\n",
    "\n",
    "def form_edge(u, v):\n",
    "    input_text = \"\".join([\"edge: \"])\n",
    "    target_text = input_text + \"\".join([u, v, \"<sep>\", v, u, \"</a>\"])\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 3  # Rough max degree\n",
    "alpha = 1.2  # Max degree flexibility factor\n",
    "T = 6  # Additional triangles per vertex\n",
    "num_nodes = 999  # Number of vertices (999 previously)\n",
    "triangle_prob = 1/3\n",
    "num_samples = 15000\n",
    "\n",
    "\n",
    "def generate_graph_with_triangles(D, alpha, T, num_nodes):\n",
    "    # Initialize the graph as an adjacency list using a dictionary\n",
    "    graph = {\"<a_{}>\".format(i): list() for i in range(num_nodes)}\n",
    "\n",
    "    # Helper function to get degree of a node\n",
    "    def degree(node):\n",
    "        return len(graph[node])\n",
    "\n",
    "    # Iterate over vertices to connect them based on the degree constraint\n",
    "    for v in graph.keys():\n",
    "        # Create a pool of all non-adjacent vertices u whose degree(u) <= alpha * D\n",
    "        non_adjacent = [u for u in graph.keys() if u != v and u not in graph[v] and degree(u) <= alpha * D]\n",
    "        \n",
    "        # Determine how many vertices to connect to v\n",
    "        needed_edges = max(0, D - degree(v))\n",
    "\n",
    "        # Randomly sample vertices from the pool\n",
    "        sampled_vertices = random.sample(non_adjacent, min(needed_edges, len(non_adjacent)))\n",
    "\n",
    "        # Add edges between v and the sampled vertices\n",
    "        for u in sampled_vertices:\n",
    "            graph[v].append(u)\n",
    "            graph[u].append(v)\n",
    "\n",
    "    # Initialize a dictionary to track the number of triangles added to each vertex\n",
    "    num_added_triangles = {node: 0 for node in graph.keys()}\n",
    "\n",
    "    # Add T triangles for each vertex\n",
    "    for u in graph.keys():\n",
    "        while num_added_triangles[u] < T:\n",
    "            # Sample two neighbors of u\n",
    "            neighbors = list(graph[u])\n",
    "            if len(neighbors) < 2:\n",
    "                print(\"Not enough neighbors to form a triangle\")\n",
    "                break  # Not enough neighbors to form a triangle\n",
    "\n",
    "            v, w = random.sample(neighbors, 2)\n",
    "\n",
    "            # Add an edge between v and w if it doesnâ€™t already exist\n",
    "            if w not in graph[v]:\n",
    "                graph[v].append(w)\n",
    "                graph[w].append(v)\n",
    "\n",
    "            # Increment the triangle count for u, v, and w\n",
    "            num_added_triangles[u] += 1\n",
    "            num_added_triangles[v] += 1\n",
    "            num_added_triangles[w] += 1\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def build_dataset(hash_str_len):\n",
    " \n",
    "    entities_vocab = [\"<a_{}>\".format(i) for i in range(num_nodes)]\n",
    "\n",
    "    edges = generate_graph_with_triangles(D, alpha, T, num_nodes)\n",
    "\n",
    "    # Instead of generating all indices at once, generate hash strings directly\n",
    "    chars = string.ascii_lowercase + string.digits\n",
    "    base = len(chars)\n",
    "    used_hashes = set()  # Keep track of used hash strings\n",
    "    \n",
    "    train_sequences, test_sequences = [], []\n",
    "    for _ in tqdm(range(num_samples)):\n",
    "        if random.random() < triangle_prob:\n",
    "            # Try to generate a triangle\n",
    "            triangle_found = False\n",
    "            while not triangle_found:\n",
    "                u = random.choice(list(edges.keys()))\n",
    "                neighbors = list(edges[u])\n",
    "                if len(neighbors) < 2:\n",
    "                    break\n",
    "\n",
    "                v, w = random.sample(neighbors, 2)\n",
    "\n",
    "                # Generate a unique hash string\n",
    "                while True:\n",
    "                    # Generate random digits and convert to hash string\n",
    "                    hash_digits = [random.randint(0, base-1) for _ in range(hash_str_len)]\n",
    "                    hash_str = ''.join(chars[d] for d in hash_digits)\n",
    "                    if hash_str not in used_hashes:\n",
    "                        used_hashes.add(hash_str)\n",
    "                        break\n",
    "\n",
    "                if w in edges[v]:\n",
    "                    train_sequences.append(form_triangle(hash_str, u, v, w))\n",
    "                    triangle_found = True\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            u = random.choice(list(edges.keys()))\n",
    "            # Generate a single edge datapoint\n",
    "            neighbors = list(edges[u])\n",
    "            if neighbors:\n",
    "                v = random.choice(neighbors)\n",
    "                train_sequences.append(form_edge(u, v))\n",
    "                \n",
    "    for _ in range(1024):\n",
    "        # Generate a unique hash string\n",
    "        while True:\n",
    "            # Generate random digits and convert to hash string\n",
    "            hash_digits = [random.randint(0, base-1) for _ in range(hash_str_len)]\n",
    "            hash_str = ''.join(chars[d] for d in hash_digits)\n",
    "            if hash_str not in used_hashes:\n",
    "                used_hashes.add(hash_str)\n",
    "                break\n",
    "            \n",
    "        test_sequences.append(form_triangle_test(hash_str))\n",
    "    \n",
    "    return entities_vocab, train_sequences, test_sequences, edges\n",
    "\n",
    "HASH_STR_LEN = 10\n",
    "\n",
    "entity_vocab, train_sequences, test_sequences, edges = build_dataset(HASH_STR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of all possible triangles\n",
    "triangle_num = 0\n",
    "for u in edges.keys():\n",
    "    neighbors = list(edges[u])\n",
    "    if len(neighbors) >= 2:\n",
    "        for v in neighbors:\n",
    "            for w in neighbors:\n",
    "                if v != w and w in edges[v]:\n",
    "                    triangle_num += 1\n",
    "print(\"triangle_num:\", triangle_num)\n",
    "\n",
    "# Get the number of all possible edges\n",
    "edge_num = 0\n",
    "for u in edges.keys():\n",
    "    edge_num += len(edges[u])\n",
    "print(\"edge_num:\", edge_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "vocab = vocab + entity_vocab\n",
    "# special tokens\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1024\n",
    "test_sequences = choose(test_sequences, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "dataset_name = \"triangle.{}\".format(HASH_STR_LEN)\n",
    "if T != 6:\n",
    "    dataset_name = dataset_name + \".T{}\".format(T)\n",
    "os.makedirs(os.path.join(DATA_ROOT, dataset_name), exist_ok=True)\n",
    "train_sequences_ds = train_sequences\n",
    "\n",
    "# Unique input_text\n",
    "input_texts = [item[\"input_text\"] for item in train_sequences_ds]\n",
    "unique_input_texts = list(set(input_texts))\n",
    "\n",
    "print(len(unique_input_texts))\n",
    "print(len(train_sequences_ds))\n",
    "\n",
    "probes = []\n",
    "for item in choose(train_sequences_ds, test_size):\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1]['type'] = 'train'\n",
    "\n",
    "for item in test_sequences:\n",
    "    probes.append(deepcopy(item))\n",
    "    probes[-1]['type'] = 'test'\n",
    "\n",
    "with open(os.path.join(DATA_ROOT, dataset_name, \"train.json\"), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(train_sequences_ds, f)\n",
    "with open(os.path.join(DATA_ROOT, dataset_name, \"valid.json\"), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(test_sequences, f)\n",
    "with open(os.path.join(DATA_ROOT, dataset_name, \"test.json\"), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(probes, f)\n",
    "# add vocab\n",
    "with open(os.path.join(DATA_ROOT, dataset_name, \"vocab.json\"), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(vocab, f)\n",
    "# add edges\n",
    "with open(os.path.join(DATA_ROOT, dataset_name, \"edges.json\"), \"w\", encoding='utf-8') as f:\n",
    "    json.dump(edges, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
