{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenwu2/anaconda3/envs/arena/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/data/locus/project_data/project_data2/chenwu2/creativity_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_creativity(hash_str, a, b1, b2):\n",
    "    input_text = \"\".join([hash_str, \"<q>\"])\n",
    "    target_text = input_text + \"\".join([b1, b2, a, \"</a>\"])\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n",
    "\n",
    "\n",
    "def form_creativity_test(hash_str, a, b1, b2):\n",
    "    input_text = \"\".join([hash_str, \"<q>\"])\n",
    "    target_text = input_text + \"\".join([\"</a>\"])  # Placeholder\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:08<00:00, 115.70it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 114.74it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 114.84it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 115.21it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 115.71it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 115.49it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 116.15it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 116.05it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 115.64it/s]\n",
      "100%|██████████| 1000/1000 [00:08<00:00, 115.28it/s]\n",
      "100%|██████████| 10/10 [01:26<00:00,  8.66s/it]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(num_a, num_b_per_a, hash_str_len):\n",
    " \n",
    "    entities_a = [\"<a_{}>\".format(i) for i in range(num_a)]\n",
    "\n",
    "    entities_b1 = [\"<b1_{}>\".format(i) for i in range(num_b_per_a * num_a)]\n",
    "    entities_b2 = [\"<b2_{}>\".format(i) for i in range(num_b_per_a * num_a)]\n",
    "\n",
    "    entity_vocab = entities_a + entities_b1 + entities_b2\n",
    "\n",
    "    entities_b1_dict = {\n",
    "        entity_a: [entities_b1[i * num_b_per_a + j] for j in range(num_b_per_a)] for i, entity_a in enumerate(entities_a)\n",
    "    }\n",
    "    entities_b2_dict = {\n",
    "        entity_a: [entities_b2[i * num_b_per_a + j] for j in range(num_b_per_a)] for i, entity_a in enumerate(entities_a)\n",
    "    }\n",
    "\n",
    "    # Instead of generating all indices at once, generate hash strings directly\n",
    "    chars = string.ascii_lowercase + string.digits\n",
    "    base = len(chars)\n",
    "    used_hashes = set()  # Keep track of used hash strings\n",
    "    \n",
    "    train_sequences, test_sequences = [], []\n",
    "    for entity_a in tqdm(entities_a):\n",
    "        entities_b1 = entities_b1_dict[entity_a]\n",
    "        entities_b2 = entities_b2_dict[entity_a]\n",
    "        for b1 in tqdm(entities_b1):\n",
    "            for b2 in entities_b2:\n",
    "                # Generate a unique hash string\n",
    "                if hash_str_len == 0:\n",
    "                    hash_str = \"\"\n",
    "                else:\n",
    "                    while True:\n",
    "                        # Generate random digits and convert to hash string\n",
    "                        hash_digits = [random.randint(0, base-1) for _ in range(hash_str_len)]\n",
    "                        hash_str = ''.join(chars[d] for d in hash_digits)\n",
    "                        if hash_str not in used_hashes:\n",
    "                            used_hashes.add(hash_str)\n",
    "                            break\n",
    "                \n",
    "                if np.random.uniform() > 0.005:\n",
    "                    train_sequences.append(form_creativity(hash_str, entity_a, b1, b2))\n",
    "                else:\n",
    "                    test_sequences.append(form_creativity_test(hash_str, entity_a, b1, b2))\n",
    "    \n",
    "    return entity_vocab, train_sequences, test_sequences, entities_b1_dict, entities_b2_dict\n",
    "\n",
    "NUM_A = 10\n",
    "NUM_B_PER_A = 1000\n",
    "HASH_STR_LEN = 10\n",
    "\n",
    "entity_vocab, train_sequences, test_sequences, entities_b1_dict, entities_b2_dict = build_dataset(NUM_A, NUM_B_PER_A, HASH_STR_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 20016\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = vocab + entity_vocab\n",
    "# special tokens\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 3000\n",
    "test_sequences = choose(test_sequences, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9950276\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: 50000\n",
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# downsampling train_inferred\n",
    "for training_size in [50000]:\n",
    "    print(f\"training size: {training_size}\")    \n",
    "    dataset_name = \"sibling.{}.{}.{}.{}\".format(NUM_A, NUM_B_PER_A, HASH_STR_LEN, training_size)\n",
    "    os.makedirs(os.path.join(DATA_ROOT, dataset_name), exist_ok=True)\n",
    "    train_sequences_ds = choose(train_sequences, training_size)\n",
    "\n",
    "    # Unique input_text\n",
    "    input_texts = [item[\"input_text\"] for item in train_sequences_ds]\n",
    "    unique_input_texts = list(set(input_texts))\n",
    "\n",
    "    print(len(unique_input_texts))\n",
    "    print(len(train_sequences_ds))\n",
    "\n",
    "    probes = []\n",
    "    for item in choose(train_sequences_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train'\n",
    "\n",
    "    for item in test_sequences:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test'\n",
    "\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"train.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(train_sequences_ds, f)\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"valid.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_sequences, f)\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"test.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # add vocab\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"vocab.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)\n",
    "    # add entities_b1_dict and entities_b2_dict\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"entities_b1_dict.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(entities_b1_dict, f)\n",
    "    with open(os.path.join(DATA_ROOT, dataset_name, \"entities_b2_dict.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(entities_b2_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
